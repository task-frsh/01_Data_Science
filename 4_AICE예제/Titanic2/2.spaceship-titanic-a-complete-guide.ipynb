{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{"editable":false}},{"cell_type":"markdown","source":"<center>\n<img src=\"https://images4.alphacoders.com/798/798114.jpg\" width=500 height=500 />\n</center>","metadata":{"editable":false}},{"cell_type":"markdown","source":"Welcome to this comprehensive guide on **binary classification** with the **Spaceship Titanic** dataset. The objective is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with a spacetime anomaly.\n\n*We will cover:*\n* Exploratory Data Analysis\n* Feature Engineering\n* Data Cleaning\n* Encoding, Scaling and Preprocessing\n* Training Machine Learning Models\n* Cross Validation and Ensembling Predictions","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{"editable":false}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nfrom imblearn.over_sampling import SMOTE\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.express as px\nimport time\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\nfrom sklearn.metrics import roc_auc_score, plot_confusion_matrix, plot_roc_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.utils import resample\n\n# Models\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:03:26.840778Z","iopub.execute_input":"2022-04-08T14:03:26.841857Z","iopub.status.idle":"2022-04-08T14:03:36.283479Z","shell.execute_reply.started":"2022-04-08T14:03:26.841691Z","shell.execute_reply":"2022-04-08T14:03:36.282618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Load data**","metadata":{"editable":false}},{"cell_type":"code","source":"# Save to df\ntrain = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest = pd.read_csv('../input/spaceship-titanic/test.csv')\n\n# Shape and preview\nprint('Train set shape:', train.shape)\nprint('Test set shape:', test.shape)\ntrain.head()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:03:36.285152Z","iopub.execute_input":"2022-04-08T14:03:36.285388Z","iopub.status.idle":"2022-04-08T14:03:36.392553Z","shell.execute_reply.started":"2022-04-08T14:03:36.285359Z","shell.execute_reply":"2022-04-08T14:03:36.391671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Feature descriptions:*\n> * **PassengerId** - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n> * **HomePlanet** - The planet the passenger departed from, typically their planet of permanent residence.\n> * **CryoSleep** - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n> * **Cabin** - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n> * **Destination** - The planet the passenger will be debarking to.\n> * **Age** - The age of the passenger.\n> * **VIP** - Whether the passenger has paid for special VIP service during the voyage.\n> * **RoomService**, **FoodCourt**, **ShoppingMall**, **Spa**, **VRDeck** - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n> * **Name** - The first and last names of the passenger.\n> * **Transported** - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Missing values**","metadata":{"editable":false}},{"cell_type":"code","source":"print('TRAIN SET MISSING VALUES:')\nprint(train.isna().sum())\nprint('')\nprint('TEST SET MISSING VALUES:')\nprint(test.isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:03.732972Z","iopub.execute_input":"2022-04-08T14:08:03.733345Z","iopub.status.idle":"2022-04-08T14:08:03.759003Z","shell.execute_reply.started":"2022-04-08T14:08:03.733309Z","shell.execute_reply":"2022-04-08T14:08:03.75802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost every feature has missing values! How we deal with these values will be very important. ","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Duplicates**","metadata":{"editable":false}},{"cell_type":"code","source":"print(f'Duplicates in train set: {train.duplicated().sum()}, ({np.round(100*train.duplicated().sum()/len(train),1)}%)')\nprint('')\nprint(f'Duplicates in test set: {test.duplicated().sum()}, ({np.round(100*test.duplicated().sum()/len(test),1)}%)')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:04.335841Z","iopub.execute_input":"2022-04-08T14:08:04.336163Z","iopub.status.idle":"2022-04-08T14:08:04.395079Z","shell.execute_reply.started":"2022-04-08T14:08:04.336126Z","shell.execute_reply":"2022-04-08T14:08:04.394128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cardinality of features**","metadata":{"editable":false}},{"cell_type":"markdown","source":"There are 6 continuous features, 4 categorical features (excluding the target) and 3 descriptive/qualitative features.","metadata":{"editable":false}},{"cell_type":"code","source":"train.nunique()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:04.891227Z","iopub.execute_input":"2022-04-08T14:08:04.891557Z","iopub.status.idle":"2022-04-08T14:08:04.914665Z","shell.execute_reply.started":"2022-04-08T14:08:04.891507Z","shell.execute_reply":"2022-04-08T14:08:04.913943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data types**","metadata":{"editable":false}},{"cell_type":"code","source":"train.dtypes","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:05.26326Z","iopub.execute_input":"2022-04-08T14:08:05.26371Z","iopub.status.idle":"2022-04-08T14:08:05.270902Z","shell.execute_reply.started":"2022-04-08T14:08:05.263671Z","shell.execute_reply":"2022-04-08T14:08:05.270284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will need to transform the data to be numeric (int64 or float64) so that we can train machine learning models. These models (in general) don't work on text.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# EDA","metadata":{"editable":false}},{"cell_type":"markdown","source":"Let us explore the dataset to gain insights. ","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Target distribution**","metadata":{"editable":false}},{"cell_type":"code","source":"# Figure size\nplt.figure(figsize=(6,6))\n\n# Pie plot\ntrain['Transported'].value_counts().plot.pie(explode=[0.1,0.1], autopct='%1.1f%%', shadow=True, textprops={'fontsize':16}).set_title(\"Target distribution\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:06.38179Z","iopub.execute_input":"2022-04-08T14:08:06.382218Z","iopub.status.idle":"2022-04-08T14:08:06.790442Z","shell.execute_reply.started":"2022-04-08T14:08:06.382186Z","shell.execute_reply":"2022-04-08T14:08:06.789033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target is highly balanced, so we luckily don't have to consider techniques like under/over-sampling.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Continuous features**","metadata":{"editable":false}},{"cell_type":"code","source":"# Figure size\nplt.figure(figsize=(10,4))\n\n# Histogram\nsns.histplot(data=train, x='Age', hue='Transported', binwidth=1, kde=True)\n\n# Aesthetics\nplt.title('Age distribution')\nplt.xlabel('Age (years)')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:06.923281Z","iopub.execute_input":"2022-04-08T14:08:06.924191Z","iopub.status.idle":"2022-04-08T14:08:07.742669Z","shell.execute_reply.started":"2022-04-08T14:08:06.92414Z","shell.execute_reply":"2022-04-08T14:08:07.74193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* 0-18 year olds were **more** likely to be transported than not.\n* 18-25 year olds were **less** likely to be transported than not.\n* Over 25 year olds were about **equally** likely to be transported than not.\n\n*Insight:*\n* Create a new feature that indicates whether the passanger is a child, adolescent or adult.","metadata":{"editable":false}},{"cell_type":"code","source":"# Expenditure features\nexp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\n# Plot expenditure features\nfig=plt.figure(figsize=(10,20))\nfor i, var_name in enumerate(exp_feats):\n    # Left plot\n    ax=fig.add_subplot(5,2,2*i+1)\n    sns.histplot(data=train, x=var_name, axes=ax, bins=30, kde=False, hue='Transported')\n    ax.set_title(var_name)\n    \n    # Right plot (truncated)\n    ax=fig.add_subplot(5,2,2*i+2)\n    sns.histplot(data=train, x=var_name, axes=ax, bins=30, kde=True, hue='Transported')\n    plt.ylim([0,100])\n    ax.set_title(var_name)\nfig.tight_layout()  # Improves appearance a bit\nplt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:07.744045Z","iopub.execute_input":"2022-04-08T14:08:07.744807Z","iopub.status.idle":"2022-04-08T14:08:11.762445Z","shell.execute_reply.started":"2022-04-08T14:08:07.744756Z","shell.execute_reply":"2022-04-08T14:08:11.761279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* Most people don't spend any money (as we can see on the left).\n* The distribution of spending decays exponentially (as we can see on the right).\n* There are a small number of outliers.\n* People who were transported tended to spend less.\n* RoomService, Spa and VRDeck have different distributions to FoodCourt and ShoppingMall - we can think of this as luxury vs essential amenities. \n\n*Insight:*\n* Create a new feature that tracks the total expenditure across all 5 amenities.\n* Create a binary feature to indicate if the person has not spent anything. (i.e. total expenditure is 0).\n* Take the log transform to reduce skew.","metadata":{}},{"cell_type":"markdown","source":"**Categorical features**","metadata":{"editable":false}},{"cell_type":"code","source":"# Categorical features\ncat_feats=['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n\n# Plot categorical features\nfig=plt.figure(figsize=(10,16))\nfor i, var_name in enumerate(cat_feats):\n    ax=fig.add_subplot(4,1,i+1)\n    sns.countplot(data=train, x=var_name, axes=ax, hue='Transported')\n    ax.set_title(var_name)\nfig.tight_layout()  # Improves appearance a bit\nplt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:11.764564Z","iopub.execute_input":"2022-04-08T14:08:11.764896Z","iopub.status.idle":"2022-04-08T14:08:12.560381Z","shell.execute_reply.started":"2022-04-08T14:08:11.764853Z","shell.execute_reply":"2022-04-08T14:08:12.559723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* VIP does not appear to be a useful feature; the target split is more or less equal. \n* CryoSleep appears the be a very useful feature in contrast.\n\n*Insights:*\n* We might consider dropping the VIP column to prevent overfitting.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Qualitative features**","metadata":{"editable":false}},{"cell_type":"markdown","source":"We can't plot this data (yet). We need to transform it into more useful features.","metadata":{"editable":false}},{"cell_type":"code","source":"# Qualitative features\nqual_feats=['PassengerId', 'Cabin' ,'Name']\n\n# Preview qualitative features\ntrain[qual_feats].head()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:12.56168Z","iopub.execute_input":"2022-04-08T14:08:12.56233Z","iopub.status.idle":"2022-04-08T14:08:12.573798Z","shell.execute_reply.started":"2022-04-08T14:08:12.562293Z","shell.execute_reply":"2022-04-08T14:08:12.57313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* PassengerId takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group.\n* Cabin takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n\n*Insights:*\n* We can extract the group and group size from the PassengerId feature. \n* We can extract the deck, number and side from the cabin feature. \n* We could extract the surname from the name feature to identify families. ","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Feature engineering","metadata":{"editable":false}},{"cell_type":"markdown","source":"The philosophy to feature engineering is simple. **Better features make better models.**","metadata":{}},{"cell_type":"markdown","source":"**Age status**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Bin age feature into groups. This will be helpful for filling missing values like expenditure according to age.","metadata":{}},{"cell_type":"code","source":"# New features - training set\ntrain['Age_group']=np.nan\ntrain.loc[train['Age']<=12,'Age_group']='Age_0-12'\ntrain.loc[(train['Age']>12) & (train['Age']<18),'Age_group']='Age_13-17'\ntrain.loc[(train['Age']>=18) & (train['Age']<=25),'Age_group']='Age_18-25'\ntrain.loc[(train['Age']>25) & (train['Age']<=30),'Age_group']='Age_26-30'\ntrain.loc[(train['Age']>30) & (train['Age']<=50),'Age_group']='Age_31-50'\ntrain.loc[train['Age']>50,'Age_group']='Age_51+'\n\n# New features - test set\ntest['Age_group']=np.nan\ntest.loc[test['Age']<=12,'Age_group']='Age_0-12'\ntest.loc[(test['Age']>12) & (test['Age']<18),'Age_group']='Age_13-17'\ntest.loc[(test['Age']>=18) & (test['Age']<=25),'Age_group']='Age_18-25'\ntest.loc[(test['Age']>25) & (test['Age']<=30),'Age_group']='Age_26-30'\ntest.loc[(test['Age']>30) & (test['Age']<=50),'Age_group']='Age_31-50'\ntest.loc[test['Age']>50,'Age_group']='Age_51+'\n\n# Plot distribution of new features\nplt.figure(figsize=(10,4))\ng=sns.countplot(data=train, x='Age_group', hue='Transported', order=['Age_0-12','Age_13-17','Age_18-25','Age_26-30','Age_31-50','Age_51+'])\nplt.title('Age group distribution')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:08:12.576074Z","iopub.execute_input":"2022-04-08T14:08:12.576307Z","iopub.status.idle":"2022-04-08T14:08:12.909807Z","shell.execute_reply.started":"2022-04-08T14:08:12.576278Z","shell.execute_reply":"2022-04-08T14:08:12.909098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Expenditure**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Calculate total expenditure and identify passengers with no expenditure.","metadata":{"editable":false}},{"cell_type":"code","source":"# New features - training set\ntrain['Expenditure']=train[exp_feats].sum(axis=1)\ntrain['No_spending']=(train['Expenditure']==0).astype(int)\n\n# New features - test set\ntest['Expenditure']=test[exp_feats].sum(axis=1)\ntest['No_spending']=(test['Expenditure']==0).astype(int)\n\n# Plot distribution of new features\nfig=plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.histplot(data=train, x='Expenditure', hue='Transported', bins=200)\nplt.title('Total expenditure (truncated)')\nplt.ylim([0,200])\nplt.xlim([0,20000])\n\nplt.subplot(1,2,2)\nsns.countplot(data=train, x='No_spending', hue='Transported')\nplt.title('No spending indicator')\nfig.tight_layout()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:08:12.910883Z","iopub.execute_input":"2022-04-08T14:08:12.911224Z","iopub.status.idle":"2022-04-08T14:08:14.866267Z","shell.execute_reply.started":"2022-04-08T14:08:12.911188Z","shell.execute_reply":"2022-04-08T14:08:14.865622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Passenger group**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Extract passenger group and group size from PassengerId.","metadata":{"editable":false}},{"cell_type":"code","source":"# New feature - Group\ntrain['Group'] = train['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\ntest['Group'] = test['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n\n# New feature - Group size\ntrain['Group_size']=train['Group'].map(lambda x: pd.concat([train['Group'], test['Group']]).value_counts()[x])\ntest['Group_size']=test['Group'].map(lambda x: pd.concat([train['Group'], test['Group']]).value_counts()[x])\n\n# Plot distribution of new features\nplt.figure(figsize=(20,4))\nplt.subplot(1,2,1)\nsns.histplot(data=train, x='Group', hue='Transported', binwidth=1)\nplt.title('Group')\n\nplt.subplot(1,2,2)\nsns.countplot(data=train, x='Group_size', hue='Transported')\nplt.title('Group size')\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:08:14.86778Z","iopub.execute_input":"2022-04-08T14:08:14.868115Z","iopub.status.idle":"2022-04-08T14:09:28.983942Z","shell.execute_reply.started":"2022-04-08T14:08:14.868082Z","shell.execute_reply":"2022-04-08T14:09:28.983123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can't really use the Group feature in our models because it has too big of a cardinality (6217) and would explode the number of dimensions with one-hot encoding.\n\nThe Group size on the other hand should be a useful feature. In fact, we can compress the feature further by creating a 'Solo' column that tracks whether someone is travelling on their own or not. The figure on the right shows that group size=1 is less likely to be transported than group size>1.","metadata":{"editable":false}},{"cell_type":"code","source":"# New feature\ntrain['Solo']=(train['Group_size']==1).astype(int)\ntest['Solo']=(test['Group_size']==1).astype(int)\n\n# New feature distribution\nplt.figure(figsize=(10,4))\nsns.countplot(data=train, x='Solo', hue='Transported')\nplt.title('Passenger travelling solo or not')\nplt.ylim([0,3000])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:09:28.985063Z","iopub.execute_input":"2022-04-08T14:09:28.985653Z","iopub.status.idle":"2022-04-08T14:09:29.240447Z","shell.execute_reply.started":"2022-04-08T14:09:28.985616Z","shell.execute_reply":"2022-04-08T14:09:29.239346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cabin location**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Extract deck, number and side from cabin feature.","metadata":{"editable":false}},{"cell_type":"code","source":"# Replace NaN's with outliers for now (so we can split feature)\ntrain['Cabin'].fillna('Z/9999/Z', inplace=True)\ntest['Cabin'].fillna('Z/9999/Z', inplace=True)\n\n# New features - training set\ntrain['Cabin_deck'] = train['Cabin'].apply(lambda x: x.split('/')[0])\ntrain['Cabin_number'] = train['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\ntrain['Cabin_side'] = train['Cabin'].apply(lambda x: x.split('/')[2])\n\n# New features - test set\ntest['Cabin_deck'] = test['Cabin'].apply(lambda x: x.split('/')[0])\ntest['Cabin_number'] = test['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\ntest['Cabin_side'] = test['Cabin'].apply(lambda x: x.split('/')[2])\n\n# Put Nan's back in (we will fill these later)\ntrain.loc[train['Cabin_deck']=='Z', 'Cabin_deck']=np.nan\ntrain.loc[train['Cabin_number']==9999, 'Cabin_number']=np.nan\ntrain.loc[train['Cabin_side']=='Z', 'Cabin_side']=np.nan\ntest.loc[test['Cabin_deck']=='Z', 'Cabin_deck']=np.nan\ntest.loc[test['Cabin_number']==9999, 'Cabin_number']=np.nan\ntest.loc[test['Cabin_side']=='Z', 'Cabin_side']=np.nan\n\n# Drop Cabin (we don't need it anymore)\ntrain.drop('Cabin', axis=1, inplace=True)\ntest.drop('Cabin', axis=1, inplace=True)\n\n# Plot distribution of new features\nfig=plt.figure(figsize=(10,12))\nplt.subplot(3,1,1)\nsns.countplot(data=train, x='Cabin_deck', hue='Transported', order=['A','B','C','D','E','F','G','T'])\nplt.title('Cabin deck')\n\nplt.subplot(3,1,2)\nsns.histplot(data=train, x='Cabin_number', hue='Transported',binwidth=20)\nplt.vlines(300, ymin=0, ymax=200, color='black')\nplt.vlines(600, ymin=0, ymax=200, color='black')\nplt.vlines(900, ymin=0, ymax=200, color='black')\nplt.vlines(1200, ymin=0, ymax=200, color='black')\nplt.vlines(1500, ymin=0, ymax=200, color='black')\nplt.vlines(1800, ymin=0, ymax=200, color='black')\nplt.title('Cabin number')\nplt.xlim([0,2000])\n\nplt.subplot(3,1,3)\nsns.countplot(data=train, x='Cabin_side', hue='Transported')\nplt.title('Cabin side')\nfig.tight_layout()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:09:29.24203Z","iopub.execute_input":"2022-04-08T14:09:29.242262Z","iopub.status.idle":"2022-04-08T14:09:30.80439Z","shell.execute_reply.started":"2022-04-08T14:09:29.242235Z","shell.execute_reply":"2022-04-08T14:09:30.803408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*This is interesting!* It appears that Cabin_number is grouped into chunks of 300 cabins. This means we can compress this feature into a categorical one, which indicates which chunk each passenger is in.\n\n*Other notes:* The cabin deck 'T' seems to be an outlier (there are only 5 samples).","metadata":{"editable":false}},{"cell_type":"code","source":"# New features - training set\ntrain['Cabin_region1']=(train['Cabin_number']<300).astype(int)   # one-hot encoding\ntrain['Cabin_region2']=((train['Cabin_number']>=300) & (train['Cabin_number']<600)).astype(int)\ntrain['Cabin_region3']=((train['Cabin_number']>=600) & (train['Cabin_number']<900)).astype(int)\ntrain['Cabin_region4']=((train['Cabin_number']>=900) & (train['Cabin_number']<1200)).astype(int)\ntrain['Cabin_region5']=((train['Cabin_number']>=1200) & (train['Cabin_number']<1500)).astype(int)\ntrain['Cabin_region6']=((train['Cabin_number']>=1500) & (train['Cabin_number']<1800)).astype(int)\ntrain['Cabin_region7']=(train['Cabin_number']>=1800).astype(int)\n\n# New features - test set\ntest['Cabin_region1']=(test['Cabin_number']<300).astype(int)   # one-hot encoding\ntest['Cabin_region2']=((test['Cabin_number']>=300) & (test['Cabin_number']<600)).astype(int)\ntest['Cabin_region3']=((test['Cabin_number']>=600) & (test['Cabin_number']<900)).astype(int)\ntest['Cabin_region4']=((test['Cabin_number']>=900) & (test['Cabin_number']<1200)).astype(int)\ntest['Cabin_region5']=((test['Cabin_number']>=1200) & (test['Cabin_number']<1500)).astype(int)\ntest['Cabin_region6']=((test['Cabin_number']>=1500) & (test['Cabin_number']<1800)).astype(int)\ntest['Cabin_region7']=(test['Cabin_number']>=1800).astype(int)\n\n# Plot distribution of new features\nplt.figure(figsize=(10,4))\ntrain['Cabin_regions_plot']=(train['Cabin_region1']+2*train['Cabin_region2']+3*train['Cabin_region3']+4*train['Cabin_region4']+5*train['Cabin_region5']+6*train['Cabin_region6']+7*train['Cabin_region7']).astype(int)\nsns.countplot(data=train, x='Cabin_regions_plot', hue='Transported')\nplt.title('Cabin regions')\ntrain.drop('Cabin_regions_plot', axis=1, inplace=True)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:09:30.805868Z","iopub.execute_input":"2022-04-08T14:09:30.806136Z","iopub.status.idle":"2022-04-08T14:09:31.164988Z","shell.execute_reply.started":"2022-04-08T14:09:30.806105Z","shell.execute_reply":"2022-04-08T14:09:31.163996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Last name**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Calculate family size from last name.","metadata":{"editable":false}},{"cell_type":"code","source":"# Replace NaN's with outliers for now (so we can split feature)\ntrain['Name'].fillna('Unknown Unknown', inplace=True)\ntest['Name'].fillna('Unknown Unknown', inplace=True)\n\n# New feature - Surname\ntrain['Surname']=train['Name'].str.split().str[-1]\ntest['Surname']=test['Name'].str.split().str[-1]\n\n# New feature - Family size\ntrain['Family_size']=train['Surname'].map(lambda x: pd.concat([train['Surname'],test['Surname']]).value_counts()[x])\ntest['Family_size']=test['Surname'].map(lambda x: pd.concat([train['Surname'],test['Surname']]).value_counts()[x])\n\n# Put Nan's back in (we will fill these later)\ntrain.loc[train['Surname']=='Unknown','Surname']=np.nan\ntrain.loc[train['Family_size']>100,'Family_size']=np.nan\ntest.loc[test['Surname']=='Unknown','Surname']=np.nan\ntest.loc[test['Family_size']>100,'Family_size']=np.nan\n\n# Drop name (we don't need it anymore)\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)\n\n# New feature distribution\nplt.figure(figsize=(12,4))\nsns.countplot(data=train, x='Family_size', hue='Transported')\nplt.title('Family size')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:09:31.167356Z","iopub.execute_input":"2022-04-08T14:09:31.168171Z","iopub.status.idle":"2022-04-08T14:10:25.770456Z","shell.execute_reply.started":"2022-04-08T14:09:31.168131Z","shell.execute_reply":"2022-04-08T14:10:25.769773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing values","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Combine train and test**","metadata":{"editable":false}},{"cell_type":"markdown","source":"This will make it easier to fill missing values. We will split it back later.","metadata":{"editable":false}},{"cell_type":"code","source":"# Labels and features\ny=train['Transported'].copy().astype(int)\nX=train.drop('Transported', axis=1).copy()\n\n# Concatenate dataframes\ndata=pd.concat([X, test], axis=0).reset_index(drop=True)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:25.771778Z","iopub.execute_input":"2022-04-08T14:10:25.772013Z","iopub.status.idle":"2022-04-08T14:10:25.798489Z","shell.execute_reply.started":"2022-04-08T14:10:25.771985Z","shell.execute_reply":"2022-04-08T14:10:25.797599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explore missing values**","metadata":{"editable":false}},{"cell_type":"code","source":"# Columns with missing values\nna_cols=data.columns[data.isna().any()].tolist()\n\n# Missing values summary\nmv=pd.DataFrame(data[na_cols].isna().sum(), columns=['Number_missing'])\nmv['Percentage_missing']=np.round(100*mv['Number_missing']/len(data),2)\nmv","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:25.799968Z","iopub.execute_input":"2022-04-08T14:10:25.800229Z","iopub.status.idle":"2022-04-08T14:10:25.847462Z","shell.execute_reply.started":"2022-04-08T14:10:25.800198Z","shell.execute_reply":"2022-04-08T14:10:25.846624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Heatmap of missing values\nplt.figure(figsize=(12,6))\nsns.heatmap(train[na_cols].isna().T, cmap='summer')\nplt.title('Heatmap of missing values')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:25.848982Z","iopub.execute_input":"2022-04-08T14:10:25.849292Z","iopub.status.idle":"2022-04-08T14:10:27.261252Z","shell.execute_reply.started":"2022-04-08T14:10:25.849251Z","shell.execute_reply":"2022-04-08T14:10:27.260346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values make up about 2% of the data, which is a relatively small amount. For the most part, they don't seem to be happening at the same time (except the features made from splitting Cabin and Name), but let's inspect closer.","metadata":{"editable":false}},{"cell_type":"code","source":"# Countplot of number of missing values by passenger\ntrain['na_count']=train.isna().sum(axis=1)\nplt.figure(figsize=(10,4))\nsns.countplot(data=train, x='na_count', hue='Transported')\nplt.title('Number of missing entries by passenger')\ntrain.drop('na_count', axis=1, inplace=True)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:27.262617Z","iopub.execute_input":"2022-04-08T14:10:27.262876Z","iopub.status.idle":"2022-04-08T14:10:27.605629Z","shell.execute_reply.started":"2022-04-08T14:10:27.262843Z","shell.execute_reply":"2022-04-08T14:10:27.604756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* Missing values are independent of the target and for the most part are isolated. \n* Even though only 2% of the data is missing, about 25% of all passengers have at least 1 missing value.\n* PassengerId is the only (original) feature to not have any missing values. \n\n\n*Insight:*\n* Since most of the missing values are isolated it makes sense to try to fill these in as opposed to just dropping rows.\n* If there is a relationship between PassengerId and other features we can fill missing values according to this column.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Strategy**","metadata":{"editable":false}},{"cell_type":"markdown","source":"The **easiest** way to deal with missing values is to just use the **median** for continuous features and the **mode** for categorical features (see version 20 of this notebook). This will work 'well enough' but if we want to maximise the accuracy of our models then we need to look for patterns within the missing data. The way to do this is by looking at the **joint distribution** of features, e.g. do passengers from the same group tend to come from the same family? There are obviously many combinations so we will just summarise the useful trends I and others have found.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**HomePlanet and Group**","metadata":{"editable":false}},{"cell_type":"code","source":"# Joint distribution of Group and HomePlanet\nGHP_gb=data.groupby(['Group','HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\nGHP_gb.head()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:27.60688Z","iopub.execute_input":"2022-04-08T14:10:27.607105Z","iopub.status.idle":"2022-04-08T14:10:27.63545Z","shell.execute_reply.started":"2022-04-08T14:10:27.607072Z","shell.execute_reply":"2022-04-08T14:10:27.634703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Countplot of unique values\nsns.countplot((GHP_gb>0).sum(axis=1))\nplt.title('Number of unique home planets per group')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:27.636431Z","iopub.execute_input":"2022-04-08T14:10:27.63668Z","iopub.status.idle":"2022-04-08T14:10:27.814762Z","shell.execute_reply.started":"2022-04-08T14:10:27.63665Z","shell.execute_reply":"2022-04-08T14:10:27.814027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Great!** This shows that everyone in the same group comes from the same home planet. So we can fill the missing HomePlanet values according to the group. (At least the ones where the group size is bigger than 1.)","metadata":{"editable":false}},{"cell_type":"code","source":"# Missing values before\nHP_bef=data['HomePlanet'].isna().sum()\n\n# Passengers with missing HomePlanet and in a group with known HomePlanet\nGHP_index=data[data['HomePlanet'].isna()][(data[data['HomePlanet'].isna()]['Group']).isin(GHP_gb.index)].index\n\n# Fill corresponding missing values\ndata.loc[GHP_index,'HomePlanet']=data.iloc[GHP_index,:]['Group'].map(lambda x: GHP_gb.idxmax(axis=1)[x])\n\n# Print number of missing values left\nprint('#HomePlanet missing values before:',HP_bef)\nprint('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:27.815829Z","iopub.execute_input":"2022-04-08T14:10:27.816138Z","iopub.status.idle":"2022-04-08T14:10:29.885421Z","shell.execute_reply.started":"2022-04-08T14:10:27.81611Z","shell.execute_reply":"2022-04-08T14:10:29.884321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We managed to fill 131 values with 100% confidence but we are not finished yet.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**HomePlanet and CabinDeck**","metadata":{"editable":false}},{"cell_type":"code","source":"# Joint distribution of CabinDeck and HomePlanet\nCDHP_gb=data.groupby(['Cabin_deck','HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n\n# Heatmap of missing values\nplt.figure(figsize=(10,4))\nsns.heatmap(CDHP_gb.T, annot=True, fmt='g', cmap='coolwarm')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:29.8869Z","iopub.execute_input":"2022-04-08T14:10:29.887576Z","iopub.status.idle":"2022-04-08T14:10:30.249796Z","shell.execute_reply.started":"2022-04-08T14:10:29.887508Z","shell.execute_reply":"2022-04-08T14:10:30.249015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* Passengers on decks A, B, C or T came from Europa.\n* Passengers on deck G came from Earth.\n* Passengers on decks D, E or F came from multiple planets.","metadata":{"editable":false}},{"cell_type":"code","source":"# Missing values before\nHP_bef=data['HomePlanet'].isna().sum()\n\n# Decks A, B, C or T came from Europa\ndata.loc[(data['HomePlanet'].isna()) & (data['Cabin_deck'].isin(['A', 'B', 'C', 'T'])), 'HomePlanet']='Europa'\n\n# Deck G came from Earth\ndata.loc[(data['HomePlanet'].isna()) & (data['Cabin_deck']=='G'), 'HomePlanet']='Earth'\n\n# Print number of missing values left\nprint('#HomePlanet missing values before:',HP_bef)\nprint('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:30.251072Z","iopub.execute_input":"2022-04-08T14:10:30.251315Z","iopub.status.idle":"2022-04-08T14:10:30.275359Z","shell.execute_reply.started":"2022-04-08T14:10:30.251284Z","shell.execute_reply":"2022-04-08T14:10:30.274439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**HomePlanet and Surname**","metadata":{"editable":false}},{"cell_type":"code","source":"# Joint distribution of Surname and HomePlanet\nSHP_gb=data.groupby(['Surname','HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n\n# Countplot of unique values\nplt.figure(figsize=(10,4))\nsns.countplot((SHP_gb>0).sum(axis=1))\nplt.title('Number of unique planets per surname')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:30.276554Z","iopub.execute_input":"2022-04-08T14:10:30.276812Z","iopub.status.idle":"2022-04-08T14:10:30.477213Z","shell.execute_reply.started":"2022-04-08T14:10:30.276783Z","shell.execute_reply":"2022-04-08T14:10:30.476217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fantastic!** Everyone with the same surname comes from the same home planet.","metadata":{"editable":false}},{"cell_type":"code","source":"# Missing values before\nHP_bef=data['HomePlanet'].isna().sum()\n\n# Passengers with missing HomePlanet and in a family with known HomePlanet\nSHP_index=data[data['HomePlanet'].isna()][(data[data['HomePlanet'].isna()]['Surname']).isin(SHP_gb.index)].index\n\n# Fill corresponding missing values\ndata.loc[SHP_index,'HomePlanet']=data.iloc[SHP_index,:]['Surname'].map(lambda x: SHP_gb.idxmax(axis=1)[x])\n\n# Print number of missing values left\nprint('#HomePlanet missing values before:',HP_bef)\nprint('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:30.479324Z","iopub.execute_input":"2022-04-08T14:10:30.480275Z","iopub.status.idle":"2022-04-08T14:10:30.89671Z","shell.execute_reply.started":"2022-04-08T14:10:30.480231Z","shell.execute_reply":"2022-04-08T14:10:30.895554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only 10 HomePlanet missing values left - let's look at them\ndata[data['HomePlanet'].isna()][['PassengerId','HomePlanet','Destination']]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:30.897926Z","iopub.execute_input":"2022-04-08T14:10:30.898154Z","iopub.status.idle":"2022-04-08T14:10:30.913641Z","shell.execute_reply.started":"2022-04-08T14:10:30.898127Z","shell.execute_reply":"2022-04-08T14:10:30.912678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Everyone left is heading towards TRAPPIST-1e. So let's look at the joint distribution of HomePlanet and Destination.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**HomePlanet and Destination**","metadata":{"editable":false}},{"cell_type":"code","source":"# Joint distribution of HomePlanet and Destination\nHPD_gb=data.groupby(['HomePlanet','Destination'])['Destination'].size().unstack().fillna(0)\n\n# Heatmap of missing values\nplt.figure(figsize=(10,4))\nsns.heatmap(HPD_gb.T, annot=True, fmt='g', cmap='coolwarm')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:30.914872Z","iopub.execute_input":"2022-04-08T14:10:30.915116Z","iopub.status.idle":"2022-04-08T14:10:31.221707Z","shell.execute_reply.started":"2022-04-08T14:10:30.915084Z","shell.execute_reply":"2022-04-08T14:10:31.220974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most people heading towards TRAPPIST-1e came from Earth so it makes sense to guess they came from there. But remember from earlier, no one on deck D came from Earth so we need to filter these out.","metadata":{"editable":false}},{"cell_type":"code","source":"# Missing values before\nHP_bef=data['HomePlanet'].isna().sum()\n\n# Fill remaining HomePlanet missing values with Earth (if not on deck D) or Mars (if on Deck D)\ndata.loc[(data['HomePlanet'].isna()) & ~(data['Cabin_deck']=='D'), 'HomePlanet']='Earth'\ndata.loc[(data['HomePlanet'].isna()) & (data['Cabin_deck']=='D'), 'HomePlanet']='Mars'\n\n# Print number of missing values left\nprint('#HomePlanet missing values before:',HP_bef)\nprint('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:31.223063Z","iopub.execute_input":"2022-04-08T14:10:31.223494Z","iopub.status.idle":"2022-04-08T14:10:31.245098Z","shell.execute_reply.started":"2022-04-08T14:10:31.223462Z","shell.execute_reply":"2022-04-08T14:10:31.24429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Awesome!** We're done with HomePlanet.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Destination**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Since the majority (68%) of passengers are heading towards TRAPPIST-1e (see EDA section), we'll just impute this value (i.e. the mode). A better rule hasn't been found at this stage.","metadata":{"editable":false}},{"cell_type":"code","source":"# Missing values before\nD_bef=data['Destination'].isna().sum()\n\n# Fill missing Destination values with mode\ndata.loc[(data['Destination'].isna()), 'Destination']='TRAPPIST-1e'\n\n# Print number of missing values left\nprint('#Destination missing values before:',D_bef)\nprint('#Destination missing values after:',data['Destination'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:10:31.246368Z","iopub.execute_input":"2022-04-08T14:10:31.24665Z","iopub.status.idle":"2022-04-08T14:10:31.260952Z","shell.execute_reply.started":"2022-04-08T14:10:31.246618Z","shell.execute_reply":"2022-04-08T14:10:31.259818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Surname and group**","metadata":{}},{"cell_type":"markdown","source":"\nThe reason we are filling missing surnames is because we will use surnames later to fill missing values of other features. It also means we can improve the accuracy of the family size feature.","metadata":{}},{"cell_type":"code","source":"# Joint distribution of Group and Surname\nGSN_gb=data[data['Group_size']>1].groupby(['Group','Surname'])['Surname'].size().unstack().fillna(0)\n\n# Countplot of unique values\nplt.figure(figsize=(10,4))\nsns.countplot((GSN_gb>0).sum(axis=1))\nplt.title('Number of unique surnames by group')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:10:31.266876Z","iopub.execute_input":"2022-04-08T14:10:31.267738Z","iopub.status.idle":"2022-04-08T14:10:31.551078Z","shell.execute_reply.started":"2022-04-08T14:10:31.267698Z","shell.execute_reply":"2022-04-08T14:10:31.550039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The majority (83%) of groups contain only 1 family. So let's fill missing surnames according to the majority surname in that group.","metadata":{}},{"cell_type":"code","source":"# Missing values before\nSN_bef=data['Surname'].isna().sum()\n\n# Passengers with missing Surname and in a group with known majority Surname\nGSN_index=data[data['Surname'].isna()][(data[data['Surname'].isna()]['Group']).isin(GSN_gb.index)].index\n\n# Fill corresponding missing values\ndata.loc[GSN_index,'Surname']=data.iloc[GSN_index,:]['Group'].map(lambda x: GSN_gb.idxmax(axis=1)[x])\n\n# Print number of missing values left\nprint('#Surname missing values before:',SN_bef)\nprint('#Surname missing values after:',data['Surname'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:10:31.552492Z","iopub.execute_input":"2022-04-08T14:10:31.552856Z","iopub.status.idle":"2022-04-08T14:10:35.542615Z","shell.execute_reply.started":"2022-04-08T14:10:31.552803Z","shell.execute_reply":"2022-04-08T14:10:35.541663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That is the best we can do. We don't have to get rid of all of these missing values because we will end up dropping the surname feature anyway. However, we can update the family size feature.","metadata":{}},{"cell_type":"code","source":"# Replace NaN's with outliers (so we can use map)\ndata['Surname'].fillna('Unknown', inplace=True)\n\n# Update family size feature\ndata['Family_size']=data['Surname'].map(lambda x: data['Surname'].value_counts()[x])\n\n# Put NaN's back in place of outliers\ndata.loc[data['Surname']=='Unknown','Surname']=np.nan\n\n# Say unknown surname means no family\ndata.loc[data['Family_size']>100,'Family_size']=0","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:10:35.543773Z","iopub.execute_input":"2022-04-08T14:10:35.543992Z","iopub.status.idle":"2022-04-08T14:11:23.367209Z","shell.execute_reply.started":"2022-04-08T14:10:35.543965Z","shell.execute_reply":"2022-04-08T14:11:23.366347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CabinSide and Group**","metadata":{"editable":false}},{"cell_type":"code","source":"# Joint distribution of Group and Cabin features\nGCD_gb=data[data['Group_size']>1].groupby(['Group','Cabin_deck'])['Cabin_deck'].size().unstack().fillna(0)\nGCN_gb=data[data['Group_size']>1].groupby(['Group','Cabin_number'])['Cabin_number'].size().unstack().fillna(0)\nGCS_gb=data[data['Group_size']>1].groupby(['Group','Cabin_side'])['Cabin_side'].size().unstack().fillna(0)\n\n# Countplots\nfig=plt.figure(figsize=(16,4))\nplt.subplot(1,3,1)\nsns.countplot((GCD_gb>0).sum(axis=1))\nplt.title('#Unique cabin decks per group')\n\nplt.subplot(1,3,2)\nsns.countplot((GCN_gb>0).sum(axis=1))\nplt.title('#Unique cabin numbers per group')\n\nplt.subplot(1,3,3)\nsns.countplot((GCS_gb>0).sum(axis=1))\nplt.title('#Unique cabin sides per group')\nfig.tight_layout()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:23.368769Z","iopub.execute_input":"2022-04-08T14:11:23.369098Z","iopub.status.idle":"2022-04-08T14:11:23.909253Z","shell.execute_reply.started":"2022-04-08T14:11:23.369049Z","shell.execute_reply":"2022-04-08T14:11:23.908423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Another rule!** Everyone in the same group is also on the same cabin side. For cabin deck and cabin number there is also a fairly good (but not perfect) correlation with group.","metadata":{"editable":false}},{"cell_type":"code","source":"# Missing values before\nCS_bef=data['Cabin_side'].isna().sum()\n\n# Passengers with missing Cabin side and in a group with known Cabin side\nGCS_index=data[data['Cabin_side'].isna()][(data[data['Cabin_side'].isna()]['Group']).isin(GCS_gb.index)].index\n\n# Fill corresponding missing values\ndata.loc[GCS_index,'Cabin_side']=data.iloc[GCS_index,:]['Group'].map(lambda x: GCS_gb.idxmax(axis=1)[x])\n\n# Print number of missing values left\nprint('#Cabin_side missing values before:',CS_bef)\nprint('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:23.910823Z","iopub.execute_input":"2022-04-08T14:11:23.9113Z","iopub.status.idle":"2022-04-08T14:11:24.515033Z","shell.execute_reply.started":"2022-04-08T14:11:23.911257Z","shell.execute_reply":"2022-04-08T14:11:24.514028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CabinSide and Surname**","metadata":{"editable":false}},{"cell_type":"code","source":"# Joint distribution of Surname and Cabin side\nSCS_gb=data[data['Group_size']>1].groupby(['Surname','Cabin_side'])['Cabin_side'].size().unstack().fillna(0)\n\n# Ratio of sides\nSCS_gb['Ratio']=SCS_gb['P']/(SCS_gb['P']+SCS_gb['S'])\n\n# Histogram of ratio\nplt.figure(figsize=(10,4))\nsns.histplot(SCS_gb['Ratio'], kde=True, binwidth=0.05)\nplt.title('Ratio of cabin side by surname')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:24.516593Z","iopub.execute_input":"2022-04-08T14:11:24.516902Z","iopub.status.idle":"2022-04-08T14:11:24.883673Z","shell.execute_reply.started":"2022-04-08T14:11:24.516857Z","shell.execute_reply":"2022-04-08T14:11:24.88284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print proportion\nprint('Percentage of families all on the same cabin side:', 100*np.round((SCS_gb['Ratio'].isin([0,1])).sum()/len(SCS_gb),3),'%')\n\n# Another view of the same information\nSCS_gb.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:24.884981Z","iopub.execute_input":"2022-04-08T14:11:24.885212Z","iopub.status.idle":"2022-04-08T14:11:24.901983Z","shell.execute_reply.started":"2022-04-08T14:11:24.885184Z","shell.execute_reply":"2022-04-08T14:11:24.900886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows that families tend to be on the same cabin side (and 77% of families are entirely on the same side).","metadata":{}},{"cell_type":"code","source":"# Missing values before\nCS_bef=data['Cabin_side'].isna().sum()\n\n# Drop ratio column\nSCS_gb.drop('Ratio', axis=1, inplace=True)\n\n# Passengers with missing Cabin side and in a family with known Cabin side\nSCS_index=data[data['Cabin_side'].isna()][(data[data['Cabin_side'].isna()]['Surname']).isin(SCS_gb.index)].index\n\n# Fill corresponding missing values\ndata.loc[SCS_index,'Cabin_side']=data.iloc[SCS_index,:]['Surname'].map(lambda x: SCS_gb.idxmax(axis=1)[x])\n\n# Drop surname (we don't need it anymore)\ndata.drop('Surname', axis=1, inplace=True)\n\n# Print number of missing values left\nprint('#Cabin_side missing values before:',CS_bef)\nprint('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:24.903447Z","iopub.execute_input":"2022-04-08T14:11:24.904091Z","iopub.status.idle":"2022-04-08T14:11:25.265093Z","shell.execute_reply.started":"2022-04-08T14:11:24.90404Z","shell.execute_reply":"2022-04-08T14:11:25.264146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The remaining missing values will be replaced with an outlier. This is because we really don't know which one of the two (balanced) sides we should assign.","metadata":{}},{"cell_type":"code","source":"# Value counts\ndata['Cabin_side'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:25.266793Z","iopub.execute_input":"2022-04-08T14:11:25.267113Z","iopub.status.idle":"2022-04-08T14:11:25.276873Z","shell.execute_reply.started":"2022-04-08T14:11:25.267067Z","shell.execute_reply":"2022-04-08T14:11:25.275972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values before\nCS_bef=data['Cabin_side'].isna().sum()\n\n# Fill remaining missing values with outlier\ndata.loc[data['Cabin_side'].isna(),'Cabin_side']='Z'\n\n# Print number of missing values left\nprint('#Cabin_side missing values before:',CS_bef)\nprint('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:25.278269Z","iopub.execute_input":"2022-04-08T14:11:25.278509Z","iopub.status.idle":"2022-04-08T14:11:25.297868Z","shell.execute_reply.started":"2022-04-08T14:11:25.278479Z","shell.execute_reply":"2022-04-08T14:11:25.296851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CabinDeck and Group**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Remember (from above) that groups tend to be on the same cabin deck.","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:39:33.520434Z","iopub.execute_input":"2022-04-06T15:39:33.52086Z","iopub.status.idle":"2022-04-06T15:39:33.52695Z","shell.execute_reply.started":"2022-04-06T15:39:33.520822Z","shell.execute_reply":"2022-04-06T15:39:33.525883Z"},"editable":false}},{"cell_type":"code","source":"# Missing values before\nCD_bef=data['Cabin_deck'].isna().sum()\n\n# Passengers with missing Cabin deck and in a group with known majority Cabin deck\nGCD_index=data[data['Cabin_deck'].isna()][(data[data['Cabin_deck'].isna()]['Group']).isin(GCD_gb.index)].index\n\n# Fill corresponding missing values\ndata.loc[GCD_index,'Cabin_deck']=data.iloc[GCD_index,:]['Group'].map(lambda x: GCD_gb.idxmax(axis=1)[x])\n\n# Print number of missing values left\nprint('#Cabin_deck missing values before:',CD_bef)\nprint('#Cabin_deck missing values after:',data['Cabin_deck'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:25.299111Z","iopub.execute_input":"2022-04-08T14:11:25.29935Z","iopub.status.idle":"2022-04-08T14:11:25.92246Z","shell.execute_reply.started":"2022-04-08T14:11:25.29932Z","shell.execute_reply":"2022-04-08T14:11:25.921174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CabinDeck and HomePlanet**","metadata":{"editable":false}},{"cell_type":"code","source":"# Joint distribution\ndata.groupby(['HomePlanet','Destination','Solo','Cabin_deck'])['Cabin_deck'].size().unstack().fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:25.923893Z","iopub.execute_input":"2022-04-08T14:11:25.924409Z","iopub.status.idle":"2022-04-08T14:11:25.967403Z","shell.execute_reply.started":"2022-04-08T14:11:25.924357Z","shell.execute_reply":"2022-04-08T14:11:25.966446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes:*\n* Passengers from Mars are most likely in deck F.\n* Passengers from Europa are (more or less) most likely in deck C if travelling solo and deck B otherwise.\n* Passengers from Earth are (more or less) most likely in deck G.","metadata":{}},{"cell_type":"markdown","source":"We will fill in missing values according to where the mode appears in these subgroups.","metadata":{}},{"cell_type":"code","source":"# Missing values before\nCD_bef=data['Cabin_deck'].isna().sum()\n\n# Fill missing values using the mode\nna_rows_CD=data.loc[data['Cabin_deck'].isna(),'Cabin_deck'].index\ndata.loc[data['Cabin_deck'].isna(),'Cabin_deck']=data.groupby(['HomePlanet','Destination','Solo'])['Cabin_deck'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))[na_rows_CD]\n\n# Print number of missing values left\nprint('#Cabin_deck missing values before:',CD_bef)\nprint('#Cabin_deck missing values after:',data['Cabin_deck'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:25.969378Z","iopub.execute_input":"2022-04-08T14:11:25.9705Z","iopub.status.idle":"2022-04-08T14:11:26.013151Z","shell.execute_reply.started":"2022-04-08T14:11:25.970465Z","shell.execute_reply":"2022-04-08T14:11:26.012248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CabinNumber and CabinDeck**","metadata":{"editable":false}},{"cell_type":"code","source":"# Scatterplot\nplt.figure(figsize=(10,4))\nsns.scatterplot(x=data['Cabin_number'], y=data['Group'], c=LabelEncoder().fit_transform(data.loc[~data['Cabin_number'].isna(),'Cabin_deck']), cmap='tab10')\nplt.title('Cabin_number vs group coloured by group')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:26.014783Z","iopub.execute_input":"2022-04-08T14:11:26.015277Z","iopub.status.idle":"2022-04-08T14:11:26.77246Z","shell.execute_reply.started":"2022-04-08T14:11:26.015231Z","shell.execute_reply":"2022-04-08T14:11:26.771864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an interesting pattern here. The cabin_number and group_number share a linear relationship on a deck by deck basis. We can therefore extrapolate the missing cabin numbers using linear regression on a deck by deck basis to get an approximate cabin number.","metadata":{}},{"cell_type":"code","source":"# Missing values before\nCN_bef=data['Cabin_number'].isna().sum()\n\n# Extrapolate linear relationship on a deck by deck basis\nfor deck in ['A', 'B', 'C', 'D', 'E', 'F', 'G']:\n    # Features and labels\n    X_CN=data.loc[~(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Group']\n    y_CN=data.loc[~(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Cabin_number']\n    X_test_CN=data.loc[(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Group']\n\n    # Linear regression\n    model_CN=LinearRegression()\n    model_CN.fit(X_CN.values.reshape(-1, 1), y_CN)\n    preds_CN=model_CN.predict(X_test_CN.values.reshape(-1, 1))\n    \n    # Fill missing values with predictions\n    data.loc[(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Cabin_number']=preds_CN.astype(int)\n\n# Print number of missing values left\nprint('#Cabin_number missing values before:',CN_bef)\nprint('#Cabin_number missing values after:',data['Cabin_number'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:26.77344Z","iopub.execute_input":"2022-04-08T14:11:26.773715Z","iopub.status.idle":"2022-04-08T14:11:26.882512Z","shell.execute_reply.started":"2022-04-08T14:11:26.773676Z","shell.execute_reply":"2022-04-08T14:11:26.88158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's update the cabin regions with the new data.","metadata":{"editable":false}},{"cell_type":"code","source":"# One-hot encode cabin regions\ndata['Cabin_region1']=(data['Cabin_number']<300).astype(int)\ndata['Cabin_region2']=((data['Cabin_number']>=300) & (data['Cabin_number']<600)).astype(int)\ndata['Cabin_region3']=((data['Cabin_number']>=600) & (data['Cabin_number']<900)).astype(int)\ndata['Cabin_region4']=((data['Cabin_number']>=900) & (data['Cabin_number']<1200)).astype(int)\ndata['Cabin_region5']=((data['Cabin_number']>=1200) & (data['Cabin_number']<1500)).astype(int)\ndata['Cabin_region6']=((data['Cabin_number']>=1500) & (data['Cabin_number']<1800)).astype(int)\ndata['Cabin_region7']=(data['Cabin_number']>=1800).astype(int)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:26.883715Z","iopub.execute_input":"2022-04-08T14:11:26.883952Z","iopub.status.idle":"2022-04-08T14:11:26.898718Z","shell.execute_reply.started":"2022-04-08T14:11:26.883923Z","shell.execute_reply":"2022-04-08T14:11:26.89762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VIP**","metadata":{"editable":false}},{"cell_type":"markdown","source":"VIP is a highly unbalanced binary feature so we will just impute the mode.","metadata":{"editable":false}},{"cell_type":"code","source":"data['VIP'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:26.900042Z","iopub.execute_input":"2022-04-08T14:11:26.902733Z","iopub.status.idle":"2022-04-08T14:11:26.914721Z","shell.execute_reply.started":"2022-04-08T14:11:26.902673Z","shell.execute_reply":"2022-04-08T14:11:26.913617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values before\nV_bef=data['VIP'].isna().sum()\n\n# Fill missing values with mode\ndata.loc[data['VIP'].isna(),'VIP']=False\n\n# Print number of missing values left\nprint('#VIP missing values before:',V_bef)\nprint('#VIP missing values after:',data['VIP'].isna().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:26.917305Z","iopub.execute_input":"2022-04-08T14:11:26.917893Z","iopub.status.idle":"2022-04-08T14:11:26.932056Z","shell.execute_reply.started":"2022-04-08T14:11:26.917852Z","shell.execute_reply":"2022-04-08T14:11:26.930959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Age**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Age varies across many features like HomePlanet, group size, expenditure and cabin deck, so we will impute missing values according to the median of these subgroups.","metadata":{}},{"cell_type":"code","source":"# Joint distribution\ndata.groupby(['HomePlanet','No_spending','Solo','Cabin_deck'])['Age'].median().unstack().fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:26.933738Z","iopub.execute_input":"2022-04-08T14:11:26.934386Z","iopub.status.idle":"2022-04-08T14:11:26.975267Z","shell.execute_reply.started":"2022-04-08T14:11:26.934339Z","shell.execute_reply":"2022-04-08T14:11:26.97435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values before\nA_bef=data[exp_feats].isna().sum().sum()\n\n# Fill missing values using the median\nna_rows_A=data.loc[data['Age'].isna(),'Age'].index\ndata.loc[data['Age'].isna(),'Age']=data.groupby(['HomePlanet','No_spending','Solo','Cabin_deck'])['Age'].transform(lambda x: x.fillna(x.median()))[na_rows_A]\n\n# Print number of missing values left\nprint('#Age missing values before:',A_bef)\nprint('#Age missing values after:',data['Age'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:26.976986Z","iopub.execute_input":"2022-04-08T14:11:26.977651Z","iopub.status.idle":"2022-04-08T14:11:27.012719Z","shell.execute_reply.started":"2022-04-08T14:11:26.977604Z","shell.execute_reply":"2022-04-08T14:11:27.011842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's update the age_group feature using the new data.","metadata":{}},{"cell_type":"code","source":"# Update age group feature\ndata.loc[data['Age']<=12,'Age_group']='Age_0-12'\ndata.loc[(data['Age']>12) & (data['Age']<18),'Age_group']='Age_13-17'\ndata.loc[(data['Age']>=18) & (data['Age']<=25),'Age_group']='Age_18-25'\ndata.loc[(data['Age']>25) & (data['Age']<=30),'Age_group']='Age_26-30'\ndata.loc[(data['Age']>30) & (data['Age']<=50),'Age_group']='Age_31-50'\ndata.loc[data['Age']>50,'Age_group']='Age_51+'","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.014311Z","iopub.execute_input":"2022-04-08T14:11:27.014844Z","iopub.status.idle":"2022-04-08T14:11:27.030243Z","shell.execute_reply.started":"2022-04-08T14:11:27.014796Z","shell.execute_reply":"2022-04-08T14:11:27.029588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CryoSleep**","metadata":{}},{"cell_type":"markdown","source":"The best way to predict if a passenger is in CryoSleep or not is to see if they spent anything.","metadata":{}},{"cell_type":"code","source":"# Joint distribution\ndata.groupby(['No_spending','CryoSleep'])['CryoSleep'].size().unstack().fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.03167Z","iopub.execute_input":"2022-04-08T14:11:27.032178Z","iopub.status.idle":"2022-04-08T14:11:27.04846Z","shell.execute_reply.started":"2022-04-08T14:11:27.032135Z","shell.execute_reply":"2022-04-08T14:11:27.047339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values before\nCSL_bef=data['CryoSleep'].isna().sum()\n\n# Fill missing values using the mode\nna_rows_CSL=data.loc[data['CryoSleep'].isna(),'CryoSleep'].index\ndata.loc[data['CryoSleep'].isna(),'CryoSleep']=data.groupby(['No_spending'])['CryoSleep'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))[na_rows_CSL]\n\n# Print number of missing values left\nprint('#CryoSleep missing values before:',CSL_bef)\nprint('#CryoSleep missing values after:',data['CryoSleep'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.049957Z","iopub.execute_input":"2022-04-08T14:11:27.050517Z","iopub.status.idle":"2022-04-08T14:11:27.079464Z","shell.execute_reply.started":"2022-04-08T14:11:27.050463Z","shell.execute_reply":"2022-04-08T14:11:27.078644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Expenditure and CryoSleep**","metadata":{"editable":false}},{"cell_type":"markdown","source":"This one makes a lot of sense. We don't expect people in CryoSleep to be able to spend anything.","metadata":{"editable":false}},{"cell_type":"code","source":"print('Maximum expenditure of passengers in CryoSleep:',data.loc[data['CryoSleep']==True,exp_feats].sum(axis=1).max())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:27.080469Z","iopub.execute_input":"2022-04-08T14:11:27.081163Z","iopub.status.idle":"2022-04-08T14:11:27.092647Z","shell.execute_reply.started":"2022-04-08T14:11:27.081119Z","shell.execute_reply":"2022-04-08T14:11:27.091614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values before\nE_bef=data[exp_feats].isna().sum().sum()\n\n# CryoSleep has no expenditure\nfor col in exp_feats:\n    data.loc[(data[col].isna()) & (data['CryoSleep']==True), col]=0\n\n# Print number of missing values left\nprint('#Expenditure missing values before:',E_bef)\nprint('#Expenditure missing values after:',data[exp_feats].isna().sum().sum())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:27.094069Z","iopub.execute_input":"2022-04-08T14:11:27.094334Z","iopub.status.idle":"2022-04-08T14:11:27.124221Z","shell.execute_reply.started":"2022-04-08T14:11:27.094303Z","shell.execute_reply":"2022-04-08T14:11:27.123318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Expenditure and others**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Expenditure varies across many features but we will only impute missing values using HomePlanet, Solo and Age group to prevent overfitting. We will also use the mean instead of the median because a large proportion of passengers don't spend anything and median usually comes out as 0. Note how under 12's don't spend anything.","metadata":{}},{"cell_type":"code","source":"# Joint distribution\ndata.groupby(['HomePlanet','Solo','Age_group'])['Expenditure'].mean().unstack().fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.125771Z","iopub.execute_input":"2022-04-08T14:11:27.126275Z","iopub.status.idle":"2022-04-08T14:11:27.150838Z","shell.execute_reply.started":"2022-04-08T14:11:27.126242Z","shell.execute_reply":"2022-04-08T14:11:27.150116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values before\nE_bef=data[exp_feats].isna().sum().sum()\n\n# Fill remaining missing values using the median\nfor col in exp_feats:\n    na_rows=data.loc[data[col].isna(),col].index\n    data.loc[data[col].isna(),col]=data.groupby(['HomePlanet','Solo','Age_group'])[col].transform(lambda x: x.fillna(x.mean()))[na_rows]\n    \n# Print number of missing values left\nprint('#Expenditure missing values before:',E_bef)\nprint('#Expenditure missing values after:',data[exp_feats].isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.151793Z","iopub.execute_input":"2022-04-08T14:11:27.152339Z","iopub.status.idle":"2022-04-08T14:11:27.257612Z","shell.execute_reply.started":"2022-04-08T14:11:27.152306Z","shell.execute_reply":"2022-04-08T14:11:27.256871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can update the expenditure and no_spending features with these new data points.","metadata":{}},{"cell_type":"code","source":"# Update expenditure and no_spending\ndata['Expenditure']=data[exp_feats].sum(axis=1)\ndata['No_spending']=(data['Expenditure']==0).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.258777Z","iopub.execute_input":"2022-04-08T14:11:27.259132Z","iopub.status.idle":"2022-04-08T14:11:27.266426Z","shell.execute_reply.started":"2022-04-08T14:11:27.259095Z","shell.execute_reply":"2022-04-08T14:11:27.265437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:27.267652Z","iopub.execute_input":"2022-04-08T14:11:27.267993Z","iopub.status.idle":"2022-04-08T14:11:27.293772Z","shell.execute_reply.started":"2022-04-08T14:11:27.267963Z","shell.execute_reply":"2022-04-08T14:11:27.292779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No missing values left!** It was a lot of effort but it should improve the accuracy of our models.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Split data back into train and test sets**","metadata":{"editable":false}},{"cell_type":"code","source":"# Train and test\nX=data[data['PassengerId'].isin(train['PassengerId'].values)].copy()\nX_test=data[data['PassengerId'].isin(test['PassengerId'].values)].copy()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:27.295283Z","iopub.execute_input":"2022-04-08T14:11:27.295667Z","iopub.status.idle":"2022-04-08T14:11:27.312094Z","shell.execute_reply.started":"2022-04-08T14:11:27.295621Z","shell.execute_reply":"2022-04-08T14:11:27.310916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop unwanted features**","metadata":{"editable":false}},{"cell_type":"code","source":"# Drop qualitative/redundant/collinear/high cardinality features\nX.drop(['PassengerId', 'Group', 'Group_size', 'Age_group', 'Cabin_number'], axis=1, inplace=True)\nX_test.drop(['PassengerId', 'Group', 'Group_size', 'Age_group', 'Cabin_number'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.313863Z","iopub.execute_input":"2022-04-08T14:11:27.314472Z","iopub.status.idle":"2022-04-08T14:11:27.324672Z","shell.execute_reply.started":"2022-04-08T14:11:27.314423Z","shell.execute_reply":"2022-04-08T14:11:27.323679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Log transform**","metadata":{}},{"cell_type":"markdown","source":"The logarithm transform is used to decrease skew in distributions, especially with large outliers. It can make it easier for algorithms to 'learn' the correct relationships. We will apply it to the expenditure features as these are heavily skewed by outliers.","metadata":{}},{"cell_type":"code","source":"# Plot log transform results\nfig=plt.figure(figsize=(12,20))\nfor i, col in enumerate(['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','Expenditure']):\n    plt.subplot(6,2,2*i+1)\n    sns.histplot(X[col], binwidth=100)\n    plt.ylim([0,200])\n    plt.title(f'{col} (original)')\n    \n    plt.subplot(6,2,2*i+2)\n    sns.histplot(np.log(1+X[col]), color='C1')\n    plt.ylim([0,200])\n    plt.title(f'{col} (log-transform)')\n    \nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:27.326657Z","iopub.execute_input":"2022-04-08T14:11:27.32729Z","iopub.status.idle":"2022-04-08T14:11:33.025215Z","shell.execute_reply.started":"2022-04-08T14:11:27.327242Z","shell.execute_reply":"2022-04-08T14:11:33.024325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply log transform\nfor col in ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','Expenditure']:\n    X[col]=np.log(1+X[col])\n    X_test[col]=np.log(1+X_test[col])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:33.026168Z","iopub.execute_input":"2022-04-08T14:11:33.02673Z","iopub.status.idle":"2022-04-08T14:11:33.039551Z","shell.execute_reply.started":"2022-04-08T14:11:33.026689Z","shell.execute_reply":"2022-04-08T14:11:33.038415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoding and scaling**","metadata":{"editable":false}},{"cell_type":"markdown","source":"We will use column transformers to be more professional. It's also good practice.","metadata":{"editable":false}},{"cell_type":"code","source":"# Indentify numerical and categorical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\ncategorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n\n# Scale numerical data to have mean=0 and variance=1\nnumerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\n# One-hot encode categorical data\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore',sparse=False))])\n\n# Combine preprocessing\nct = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)],\n        remainder='passthrough')\n\n# Apply preprocessing\nX = ct.fit_transform(X)\nX_test = ct.transform(X_test)\n\n# Print new shape\nprint('Training set shape:', X.shape)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:33.041084Z","iopub.execute_input":"2022-04-08T14:11:33.04158Z","iopub.status.idle":"2022-04-08T14:11:33.112753Z","shell.execute_reply.started":"2022-04-08T14:11:33.041515Z","shell.execute_reply":"2022-04-08T14:11:33.111848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PCA**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Just for fun, let's look at the transformed data in PCA space. This gives a low dimensional representation of the data, which preserves local and global structure.","metadata":{"editable":false}},{"cell_type":"code","source":"pca = PCA(n_components=3)\ncomponents = pca.fit_transform(X)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=y, size=0.1*np.ones(len(X)), opacity = 1,\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n    width=800, height=500\n)\nfig.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:33.114384Z","iopub.execute_input":"2022-04-08T14:11:33.114884Z","iopub.status.idle":"2022-04-08T14:11:34.4324Z","shell.execute_reply.started":"2022-04-08T14:11:33.114843Z","shell.execute_reply":"2022-04-08T14:11:34.431623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explained variance (how important each additional principal component is)\npca = PCA().fit(X)\nfig, ax = plt.subplots(figsize=(10,4))\nxi = np.arange(1, 1+X.shape[1], step=1)\nyi = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(xi, yi, marker='o', linestyle='--', color='b')\n\n# Aesthetics\nplt.ylim(0.0,1.1)\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(1, 1+X.shape[1], step=2))\nplt.ylabel('Cumulative variance (%)')\nplt.title('Explained variance by each component')\nplt.axhline(y=1, color='r', linestyle='-')\nplt.text(0.5, 0.85, '100% cut-off threshold', color = 'red')\nax.grid(axis='x')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:34.433687Z","iopub.execute_input":"2022-04-08T14:11:34.43471Z","iopub.status.idle":"2022-04-08T14:11:34.833325Z","shell.execute_reply.started":"2022-04-08T14:11:34.434659Z","shell.execute_reply":"2022-04-08T14:11:34.83248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a validation set**","metadata":{"editable":false}},{"cell_type":"markdown","source":"We will use this to choose which model(s) to use.","metadata":{"editable":false}},{"cell_type":"code","source":"# Train-validation split\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,stratify=y,train_size=0.8,test_size=0.2,random_state=0)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:34.835044Z","iopub.execute_input":"2022-04-08T14:11:34.835669Z","iopub.status.idle":"2022-04-08T14:11:34.847864Z","shell.execute_reply.started":"2022-04-08T14:11:34.835616Z","shell.execute_reply":"2022-04-08T14:11:34.847148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model selection","metadata":{"editable":false}},{"cell_type":"markdown","source":"To briefly mention the algorithms we will use,\n\n**Logistic Regression:** Unlike linear regression which uses Least Squares, this model uses Maximum Likelihood Estimation to fit a sigmoid-curve on the target variable distribution. The sigmoid/logistic curve is commonly used when the data is questions had binary output.\n\n**K-Nearest Neighbors (KNN):** KNN works by selecting the majority class of the k-nearest neighbours, where the metric used is usually Euclidean distance. It is a simple and effective algorithm but can be sensitive by many factors, e.g. the value of k, the preprocessing done to the data and the metric used.\n\n**Support Vector Machine (SVM):** SVM finds the optimal hyperplane that seperates the data in the feature space. Predictions are made by looking at which side of the hyperplane the test point lies on. Ordinary SVM assumes the data is linearly separable, which is not always the case. A kernel trick can be used when this assumption fails to transform the data into a higher dimensional space where it is linearly seperable. SVM is a popular algorithm because it is computationally effecient and produces very good results.\n\n**Random Forest (RF):** RF is a reliable ensemble of decision trees, which can be used for regression or classification problems. Here, the individual trees are built via bagging (i.e. aggregation of bootstraps which are nothing but multiple train datasets created via sampling with replacement) and split using fewer features. The resulting diverse forest of uncorrelated trees exhibits reduced variance; therefore, is more robust towards change in data and carries its prediction accuracy to new data. It works well with both continuous & categorical data.\n\n**Extreme Gradient Boosting (XGBoost):** XGBoost is similar to RF in that it is made up of an ensemble of decision-trees. The difference arises in how those trees as derived; XGboost uses extreme gradient boosting when optimising its objective function. It often produces the best results but is relatively slow compared to other gradient boosting algorithms.\n\n**Light Gradient Boosting Machine (LGBM):** LGBM works essentially the same as XGBoost but with a lighter boosting technique. It usually produces similar results to XGBoost but is significantly faster.\n\n**Categorical Boosting (CatBoost):** CatBoost is an open source algorithm based on gradient boosted decision trees. It supports numerical, categorical and text features. It works well with heterogeneous data and even relatively small data. Informally, it tries to take the best of both worlds from XGBoost and LGBM.\n\n**Naive Bayes (NB):** Naive Bayes learns how to classify samples by using Bayes' Theorem. It uses prior information to 'update' the probability of an event by incoorporateing this information according to Bayes' law. The algorithm is quite fast but a downside is that it assumes the input features are independent, which is not always the case.","metadata":{}},{"cell_type":"markdown","source":"We will train these models and evaluate them on the validation set to then choose which ones to carry through to the next stage (cross validation).","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Define classifiers**","metadata":{"editable":false}},{"cell_type":"code","source":"# Classifiers\nclassifiers = {\n    \"LogisticRegression\" : LogisticRegression(random_state=0),\n    \"KNN\" : KNeighborsClassifier(),\n    \"SVC\" : SVC(random_state=0, probability=True),\n    \"RandomForest\" : RandomForestClassifier(random_state=0),\n    #\"XGBoost\" : XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss'), # XGBoost takes too long\n    \"LGBM\" : LGBMClassifier(random_state=0),\n    \"CatBoost\" : CatBoostClassifier(random_state=0, verbose=False),\n    \"NaiveBayes\": GaussianNB()\n}\n\n# Grids for grid search\nLR_grid = {'penalty': ['l1','l2'],\n           'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n           'max_iter': [50, 100, 150]}\n\nKNN_grid = {'n_neighbors': [3, 5, 7, 9],\n            'p': [1, 2]}\n\nSVC_grid = {'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n            'kernel': ['linear', 'rbf'],\n            'gamma': ['scale', 'auto']}\n\nRF_grid = {'n_estimators': [50, 100, 150, 200, 250, 300],\n        'max_depth': [4, 6, 8, 10, 12]}\n\nboosted_grid = {'n_estimators': [50, 100, 150, 200],\n        'max_depth': [4, 8, 12],\n        'learning_rate': [0.05, 0.1, 0.15]}\n\nNB_grid={'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]}\n\n# Dictionary of all grids\ngrid = {\n    \"LogisticRegression\" : LR_grid,\n    \"KNN\" : KNN_grid,\n    \"SVC\" : SVC_grid,\n    \"RandomForest\" : RF_grid,\n    \"XGBoost\" : boosted_grid,\n    \"LGBM\" : boosted_grid,\n    \"CatBoost\" : boosted_grid,\n    \"NaiveBayes\": NB_grid\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:11:34.849324Z","iopub.execute_input":"2022-04-08T14:11:34.849818Z","iopub.status.idle":"2022-04-08T14:11:34.865577Z","shell.execute_reply.started":"2022-04-08T14:11:34.84972Z","shell.execute_reply":"2022-04-08T14:11:34.864627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train and evaluate models**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Train models with grid search (but no cross validation so it doesn't take too long) to get a rough idea of which are the best models for this dataset.","metadata":{"editable":false}},{"cell_type":"code","source":"i=0\nclf_best_params=classifiers.copy()\nvalid_scores=pd.DataFrame({'Classifer':classifiers.keys(), 'Validation accuracy': np.zeros(len(classifiers)), 'Training time': np.zeros(len(classifiers))})\nfor key, classifier in classifiers.items():\n    start = time.time()\n    clf = GridSearchCV(estimator=classifier, param_grid=grid[key], n_jobs=-1, cv=None)\n\n    # Train and score\n    clf.fit(X_train, y_train)\n    valid_scores.iloc[i,1]=clf.score(X_valid, y_valid)\n\n    # Save trained model\n    clf_best_params[key]=clf.best_params_\n    \n    # Print iteration and training time\n    stop = time.time()\n    valid_scores.iloc[i,2]=np.round((stop - start)/60, 2)\n    \n    print('Model:', key)\n    print('Training time (mins):', valid_scores.iloc[i,2])\n    print('')\n    i+=1","metadata":{"_kg_hide-output":true,"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:11:34.866916Z","iopub.execute_input":"2022-04-08T14:11:34.86732Z","iopub.status.idle":"2022-04-08T14:20:15.408316Z","shell.execute_reply.started":"2022-04-08T14:11:34.867273Z","shell.execute_reply":"2022-04-08T14:20:15.407171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show results\nvalid_scores","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:20:15.40981Z","iopub.execute_input":"2022-04-08T14:20:15.410113Z","iopub.status.idle":"2022-04-08T14:20:15.422968Z","shell.execute_reply.started":"2022-04-08T14:20:15.410078Z","shell.execute_reply":"2022-04-08T14:20:15.421819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Motivated by this, we will take LGBM and CatBoost to the final stage of modelling.","metadata":{}},{"cell_type":"code","source":"# Show best parameters from grid search\nclf_best_params","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T14:20:15.424123Z","iopub.execute_input":"2022-04-08T14:20:15.424372Z","iopub.status.idle":"2022-04-08T14:20:15.436134Z","shell.execute_reply.started":"2022-04-08T14:20:15.424342Z","shell.execute_reply":"2022-04-08T14:20:15.434833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"editable":false}},{"cell_type":"markdown","source":"We can finally train our best model on the whole training set using cross validation and ensembling predictions together to produce the most confident predictions.","metadata":{"editable":false}},{"cell_type":"markdown","source":"**Define best models**","metadata":{"editable":false}},{"cell_type":"code","source":"# Classifiers\nbest_classifiers = {\n    \"LGBM\" : LGBMClassifier(**clf_best_params[\"LGBM\"], random_state=0),\n    \"CatBoost\" : CatBoostClassifier(**clf_best_params[\"CatBoost\"], verbose=False, random_state=0),\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-08T14:20:15.437677Z","iopub.execute_input":"2022-04-08T14:20:15.43841Z","iopub.status.idle":"2022-04-08T14:20:15.450134Z","shell.execute_reply.started":"2022-04-08T14:20:15.438369Z","shell.execute_reply":"2022-04-08T14:20:15.44922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross validation and ensembling predictions**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Predictions are ensembled together using soft voting. This averages the predicted probabilies to produce the most confident predictions.","metadata":{"editable":false}},{"cell_type":"code","source":"# Number of folds in cross validation\nFOLDS=10\n\npreds=np.zeros(len(X_test))\nfor key, classifier in best_classifiers.items():\n    start = time.time()\n    \n    # 10-fold cross validation\n    cv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n    \n    score=0\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        # Get training and validation sets\n        X_train, X_valid = X[train_idx], X[val_idx]\n        y_train, y_valid = y[train_idx], y[val_idx]\n\n        # Train model\n        clf = classifier\n        clf.fit(X_train, y_train)\n\n        # Make predictions and measure accuracy\n        preds += clf.predict_proba(X_test)[:,1]\n        score += clf.score(X_valid, y_valid)\n\n    # Average accuracy    \n    score=score/FOLDS\n    \n    # Stop timer\n    stop = time.time()\n\n    # Print accuracy and time\n    print('Model:', key)\n    print('Average validation accuracy:', np.round(100*score,2))\n    print('Training time (mins):', np.round((stop - start)/60,2))\n    print('')\n    \n# Ensemble predictions\npreds=preds/(FOLDS*len(best_classifiers))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T15:03:08.662672Z","iopub.execute_input":"2022-04-08T15:03:08.663597Z","iopub.status.idle":"2022-04-08T15:03:16.88013Z","shell.execute_reply.started":"2022-04-08T15:03:08.663509Z","shell.execute_reply":"2022-04-08T15:03:16.879232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"editable":false}},{"cell_type":"markdown","source":"Let's look at the distribution of the predicted probabilities.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.histplot(preds, binwidth=0.01, kde=True)\nplt.title('Predicted probabilities')\nplt.xlabel('Probability')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T15:03:16.881935Z","iopub.execute_input":"2022-04-08T15:03:16.882674Z","iopub.status.idle":"2022-04-08T15:03:17.401716Z","shell.execute_reply.started":"2022-04-08T15:03:16.882625Z","shell.execute_reply":"2022-04-08T15:03:17.400783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is interesting to see that the models are either very confident or very unconfident but not much in between.","metadata":{}},{"cell_type":"markdown","source":"**Post processing**","metadata":{"editable":false}},{"cell_type":"markdown","source":"Finally, we need to convert each predicted probability into one of the two classes (transported or not). The simplest way is to round each probability to the nearest integer (0 for False or 1 for True). However, assuming the train and test sets have similar distributions, we can tune the classification threshold to obtain a similar proportion of transported/not transported in our predictions as in the train set. Remember that the proportion of transported passengers in the train set was 50.4%.","metadata":{}},{"cell_type":"code","source":"# Proportion (in test set) we get from rounding\nprint(np.round(100*np.round(preds).sum()/len(preds),2))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T15:06:05.166339Z","iopub.execute_input":"2022-04-08T15:06:05.166669Z","iopub.status.idle":"2022-04-08T15:06:05.172378Z","shell.execute_reply.started":"2022-04-08T15:06:05.166619Z","shell.execute_reply":"2022-04-08T15:06:05.171686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our models seem to (potentially) overestimate the number of transported passengers in the test set. Let's try to bring that proportion down a bit. ","metadata":{}},{"cell_type":"code","source":"# Proportion of predicted positive (transported) classes\ndef preds_prop(preds_arr, thresh):\n    pred_classes=(preds_arr>=thresh).astype(int)\n    return pred_classes.sum()/len(pred_classes)\n\n# Plot proportions across a range of thresholds\ndef plot_preds_prop(preds_arr):\n    # Array of thresholds\n    T_array=np.arange(0,1,0.001)\n    \n    # Calculate proportions\n    prop=np.zeros(len(T_array))\n    for i, T in enumerate(T_array):\n        prop[i]=preds_prop(preds_arr, T)\n        \n    # Plot proportions\n    plt.figure(figsize=(10,4))\n    plt.plot(T_array, prop)\n    target_prop=0.519         # Experiment with this value\n    plt.axhline(y=target_prop, color='r', linestyle='--')\n    plt.text(-0.02,0.45,f'Target proportion: {target_prop}', fontsize=14)\n    plt.title('Predicted target distribution vs threshold')\n    plt.xlabel('Threshold')\n    plt.ylabel('Proportion')\n    \n    # Find optimal threshold (the one that leads to the proportion being closest to target_prop)\n    T_opt=T_array[np.abs(prop-target_prop).argmin()]\n    print('Optimal threshold:', T_opt)\n    return T_opt\n    \nT_opt=plot_preds_prop(preds)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T15:08:05.651014Z","iopub.execute_input":"2022-04-08T15:08:05.651375Z","iopub.status.idle":"2022-04-08T15:08:05.967179Z","shell.execute_reply.started":"2022-04-08T15:08:05.651338Z","shell.execute_reply":"2022-04-08T15:08:05.965935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classify test set using optimal threshold\npreds_tuned=(preds>=T_opt).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T15:01:01.480022Z","iopub.execute_input":"2022-04-08T15:01:01.480323Z","iopub.status.idle":"2022-04-08T15:01:01.484689Z","shell.execute_reply.started":"2022-04-08T15:01:01.48029Z","shell.execute_reply":"2022-04-08T15:01:01.483602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Submit predictions**","metadata":{"editable":false}},{"cell_type":"code","source":"# Sample submission (to get right format)\nsub=pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\n\n# Add predictions\nsub['Transported']=preds_tuned\n\n# Replace 0 to False and 1 to True\nsub=sub.replace({0:False, 1:True})\n\n# Prediction distribution\nplt.figure(figsize=(6,6))\nsub['Transported'].value_counts().plot.pie(explode=[0.1,0.1], autopct='%1.1f%%', shadow=True, textprops={'fontsize':16}).set_title(\"Prediction distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:15:30.292086Z","iopub.execute_input":"2022-04-08T12:15:30.293138Z","iopub.status.idle":"2022-04-08T12:15:30.480912Z","shell.execute_reply.started":"2022-04-08T12:15:30.293085Z","shell.execute_reply":"2022-04-08T12:15:30.479483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output to csv\nsub.to_csv('submission.csv', index=False)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-08T12:15:34.464317Z","iopub.execute_input":"2022-04-08T12:15:34.465259Z","iopub.status.idle":"2022-04-08T12:15:34.48484Z","shell.execute_reply.started":"2022-04-08T12:15:34.465206Z","shell.execute_reply":"2022-04-08T12:15:34.483789Z"},"trusted":true},"execution_count":null,"outputs":[]}]}